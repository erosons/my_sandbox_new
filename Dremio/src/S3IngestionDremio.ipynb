{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.utils.dates import days_ago\n",
    "#from airflow.operators.email_operator import EmailOperator\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import logging\n",
    "\n",
    "\n",
    "\n",
    "args = {\n",
    "    \"owner\": \"Samson\",\n",
    "    \"start_date\": days_ago(1),\n",
    "    \"retries\": 1,\n",
    "    \"owner\": \"Samson\",\n",
    "    \"email\": \",\n",
    "    \"email_on_failure\": True,\n",
    "    \"email_on retry\": True,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"ELT2_Ingestion\",\n",
    "    schedule_interval=\"@daily\",\n",
    "    tags=[\"S3Ingestion_fromDremio\"],\n",
    "    default_args=args,\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "    \n",
    "    \n",
    "    sql = \"\"\"SELECT * FROM \"Mp2-Reporting\".\"Customer List\".\"odin_Customer_list_extract\".\"unpivot passthrough\" \"\"\"\n",
    "\n",
    "    local_filename = \"passthrough_pivot.csv\"\n",
    "\n",
    "    s3_file = local_filename\n",
    "\n",
    "    bucketName = \"prod-smt-data-cache\"\n",
    "\n",
    "    logging.basicConfig(filename='new.log', filemode='w', level=logging.DEBUG,\n",
    "                        format='%(asctime)s:%(levelname)s:%(message)s')\n",
    "\n",
    "    sqlcommand = \"SELECT * FROM test.Customers;\"\n",
    "\n",
    "    \n",
    "    def conn_util():\n",
    "        try:\n",
    "            host = os.getenv(\"host\")\n",
    "            port = '31010'\n",
    "            uid = os.getenv(\"uid\")\n",
    "            pwsd = os.getenv(\"pwd\")\n",
    "            #driver = '/Library/Dremio/ODBC/lib/libdrillodbc_sbu.dylib'\n",
    "            # OR\n",
    "            driver = '/opt/dremio-odbc/lib64/libdrillodbc_sb64.so'\n",
    "            # OR\n",
    "            #driver = \"{Dremio Connector}\" This is providing map in the odbcinst.ini\n",
    "            cnxn = pyodbc.connect(\"Driver={};ConnectionType=Direct;HOST={};PORT={};AuthenticationType=Plain;UID={};PWD={}\".format(\n",
    "                driver, host, port, uid, pwsd), autocommit=True)\n",
    "\n",
    "            return cnxn\n",
    "\n",
    "        except:\n",
    "            logging.debug(\"check connection variable\")\n",
    " \n",
    "    \"\"\"\n",
    "    Extraction of passthrough data from dremio and parsing to pivot\n",
    "    \"\"\"\n",
    "    def pivotCaller(sql,cxn):\n",
    "        try:\n",
    "            data = pd.read_sql(sql, cxn)\n",
    "            data['values'] = 1\n",
    "            data = data.pivot(index='id', columns='Passthroughs', values='values')\n",
    "            data = data.fillna(0)\n",
    "            data.to_csv(local_filename, index=True)\n",
    "            return logging.info(\"csv file extract was successful\")\n",
    "\n",
    "        except ValueError as v:\n",
    "            logging.debug(\"Index contains duplicate entries, cannot reshape\", v)\n",
    "    \n",
    "    \"\"\"\n",
    "    Connection to the s3 bucket\n",
    "    \"\"\"\n",
    "    def s3connection():\n",
    "        try:\n",
    "            s3 = boto3.client('s3', aws_access_key_id='',\n",
    "                              aws_secret_access_key='',region_name=\"us-east-2\")\n",
    "            logging.info('S3 connection was succesfully established'.format(s3))\n",
    "            return s3\n",
    "        except:\n",
    "            logging.debug('Chk acces_key_id , aws_secet_access_key are valid')          \n",
    " \n",
    "\n",
    "    s3connection = s3connection()\n",
    "   \n",
    "    \"\"\"\n",
    "    File upload into S3 bucket\n",
    "    \"\"\"\n",
    "    def upload_file(file_name, bucket_name, object_Name):\n",
    "        # s3_connection.upload_file(local_filename,bucket,s3_file)\n",
    "        # if object_Name is None:\n",
    "        # object_Name = s3_file\n",
    "        try:\n",
    "            response = s3connection.upload_file(\n",
    "                file_name, bucket_name, object_Name)\n",
    "        except ClientError as e:\n",
    "            logging.error(e)\n",
    "            return print(\"Failed connection and upload\")\n",
    "        return print(\"Upload successful\")\n",
    "\n",
    "\n",
    "    dremio_extract_pivotting = PythonOperator(\n",
    "            task_id=\"dremio_extract_pivotting\",\n",
    "            python_callable=pivotCaller,\n",
    "            op_kwargs={\"query\": sqlcommand,\"connection\":conn_util()},\n",
    "            dag=dag,\n",
    "        )\n",
    "\n",
    "    S3_ingestion = PythonOperator(\n",
    "            task_id=\"Uploading_file_s3\",\n",
    "            python_callable=pivotCaller,\n",
    "            op_kwargs={\"fileName\": file_name,\"bucketName\":bucket_name,Object=None},\n",
    "            dag=dag,\n",
    "        )\n",
    "\n",
    "\n",
    "    dremio_extract_pivotting >> test_bash.set_upsteam \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
