# pip install pyspark azure-storage-file-datalake

from pyspark.sql import SparkSession
from azure.storage.filedatalake import DataLakeServiceClient
import os

# Initialize a Spark session
spark = SparkSession \
     .builder \
     .appName("SQL to Data Lake") \
     .getOrCreate()


# Function to create a DataLakeServiceClient using connection string
def create_datalake_service_client(connection_string):
    return DataLakeServiceClient.from_connection_string(connection_string)


# Read data from Azure SQL Database into a Spark DataFrame
def read_sql_data(spark, jdbc_url, query, user, password):
    return (
        spark.read.format("jdbc")
        .option("url", jdbc_url)
        .option("query", query)
        .option("user", user)
        .option("password", password)
        .option("driver", "com.microsoft.sqlserver.jdbc.SQLServerDriver")
        .load()
    )


# Upload Spark DataFrame to Azure Data Lake
def upload_to_datalake(spark_df, datalake_service_client, filesystem_name, file_path):
    filesystem_client = datalake_service_client.get_file_system_client(
        file_system=filesystem_name
    )
    file_client = filesystem_client.get_file_client(file_path)

    # Create a new file in Data Lake Gen2
    file_client.create_file()

    # Collect the data and upload row by row
    data_rows = spark_df.collect()
    for row in data_rows:
        row_data = ",".join(map(str, row)) + "\n"  # Convert Spark Row to CSV line
        file_client.append_data(row_data.encode(), offset=0, length=len(row_data))

    # Flush the data to make it available in Data Lake
    file_client.flush_data(len(row_data))

    print(f"Data from SQL moved to Data Lake at {file_path}")


# JDBC URL and credentials for Azure SQL
jdbc_url = (
    "jdbc:sqlserver://your-sql-server.database.windows.net:1433;database=your-database"
)
sql_query = "SELECT * FROM your_table"
user = "your-username"
password = "your-password"

# Read the data from Azure SQL into a Spark DataFrame
spark_df = read_sql_data(spark, jdbc_url, sql_query, user, password)

# Azure Data Lake Blob Connection string
datalake_connection_string = os.getenv("AZURE_DATALAKE_CONNECTION_STRING")

# Create Data Lake service client
datalake_service_client = create_datalake_service_client(datalake_connection_string)

# Upload to Data Lake
upload_to_datalake(
    spark_df, datalake_service_client, "your-filesystem", "path/to/output-file.csv"
)
