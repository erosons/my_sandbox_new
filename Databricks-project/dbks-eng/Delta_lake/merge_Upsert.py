from delta.tables import (DataFrame,
                          DeltaTable,
                          DeltaTableBuilder,
                          DeltaOptimizeBuilder)
from databricks.connect import DatabricksSession
from pyspark.sql.functions import col,lit,expr,current_date
from pyspark.sql import Row
from delta_reader import spark
from pyspark.sql.functions import sha2, concat_ws
import json
import os
import pandas as pd

#############################################################
# When a Merge has Autogenerated column, it is important to use the following command
# FOR SQL COMMANDS
###SET spark.databricks.delta.schema.autoMerge.enabled=true; 
#################################################################

#############################################################
# FOR PYTHON  COMMANDS
###.config("spark.databricks.delta.schema.autoMerge.enabled" "true")\
#################################################################

"""
# Call the Delta table and return a delta Object
# This function updates the functions such
#  CRUD - Update, append/insert/delete, merge
"""
# Read dataframe directly from the Tuple
def dataframe_events_2():
    students2_str = """{"students": [{"student_id":4,"student_first_name":"Elyse","student_last_name":"Addionisio","student_email":"eaddionisio3@berkeley.edu","student_gender":"Polygender","student_phone_numbers":["7347984926","3364474838","7136381150"],"student_address":{"street":"77 Sugar Alley","city":"Atlanta","state":"Georgia","postal_code":"31132"}},{"student_id":5,"student_first_name":"Lilian","student_last_name":"Warret","student_email":"lwarret4@nsw.gov.au","student_gender":"Male","student_phone_numbers":["5031246553","6151432197","2152754201"],"student_address":{"street":"82540 Summer Ridge Point","city":"Sioux Falls","state":"South Dakota","postal_code":"57193"}},{"student_id":6,"student_first_name":"Tate","student_last_name":"Swyne","student_email":"tswyne5@hud.gov","student_gender":"Agender","student_phone_numbers":["2021437429","8507115330","3047568052","7818031186","6072847440"],"student_address":{"street":"23 Sommers Parkway","city":"El Paso","state":"Texas","postal_code":"88569"}},{"student_id":7,"student_first_name":"Ichabod","student_last_name":"Moring","student_email":"imoring6@un.org","student_gender":"Female","student_phone_numbers":["7147001301","9895085931"],"student_address":{"street":"584 Reindahl Way","city":"Denver","state":"Colorado","postal_code":"80228"}},{"student_id":8,"student_first_name":"Ariel","student_last_name":"Howler","student_email":"ahowler7@tinypic.com","student_gender":"Agender","student_phone_numbers":null,"student_address":{"street":null,"city":null,"state":null,"postal_code":null}},{"student_id":9,"student_first_name":"Octavia","student_last_name":"Stenner","student_email":"ostenner8@networksolutions.com","student_gender":"Bigender","student_phone_numbers":null,"student_address":{"street":null,"city":null,"state":null,"postal_code":null}},{"student_id":10,"student_first_name":"Ronda","student_last_name":"Stean","student_email":"rstean9@xrea.com","student_gender":"Genderfluid","student_phone_numbers":null,"student_address":{"street":null,"city":null,"state":null,"postal_code":null}}]}"""
    file_ouput = json.loads(students2_str)
    l = [Row(**x) for x in file_ouput["students"]]
    df = spark.createDataFrame(l)
    print(df.show())
    return df

def dataframe_events():
   file_path = '/home/samson/Desktop/my_sandbox_new/Databricks-project/sample-case_2.csv'
   df =pd.read_csv(file_path)
   d=df.to_dict(orient='records')
   df = spark.createDataFrame(d)
   df = df.withColumn("record_hash", sha2(concat_ws("||", *df.columns), 256))
   return df


delta_table = DeltaTable.forPath(spark, "s3://extertables-loc/project1/")

print(f'returns delta object ,{delta_table}')
print(f'this delta object is can used just a regular dataframe')
delta_table.toDF().show()

# Scenario 1: DeltaTable and dataframe mostly for streaming scenario as your data is arriving you are loading
def merge_scenario_1():
    delta_table.alias("target").merge(
            source = dataframe_events().alias("events"),
            condition = "events.student_id = target.student_id"
        ).whenMatchedUpdateAll() \
         .whenNotMatchedInsertAll() \
         .execute()
    return delta_table.toDF().show()

# Scenario 2: DeltaTable to Deltatable analyzing large table scenario for optimal performance
def merge_scenario_2():
    delta_table.toDF().show()
    delta_table.alias("target").merge(
            source = delta_table_2.alias("events"),
            condition = "events.student_id = target.student_id"
        ).whenMatchedUpdateAll() \
        .whenNotMatchedInsertAll()\
            .execute()
    return delta_table.toDF().show()

# Partial Merge/Updae Operations

# Example 2 With conditions and update expressions as Spark SQL functions:
def partial_merge():
    delta_table.toDF().show()
    delta_table.alias("target").merge(
            source = delta_table_2.alias("events"),
            condition = "events.student_id = target.student_id"
        ).whenMatchedUpdate(set =
        {
          "student_email" : col("events.student_email")
        }
      ).whenNotMatchedInsert(values =
        {
          "student_id": col("events.student_id"),
          "student_first_name": col("events.student_first_name"),
          "student_last_name": col("events.student_gender"),
          "student_gender": col("events.student_last_name"),
          "student_phone_numbers": col("events.student_phone_numbers"),
        }
      ).execute()
    

    
def scd_merge():
    delta_table.toDF().show()
    # Assuming delta_table is already loaded as a DeltaTable object
    # Create a DataFrame from source data with a hash column
    # Assuming delta_table is your target Delta table and dataframe_events() returns your source DataFrame
    delta_table.alias("target").merge(
        source=dataframe_events().alias("source"),
        condition="target.student_id = source.student_id"
    ).whenMatchedUpdate(
        condition=""" 
            target.is_current = 'Y' AND
            target.record_hash <> source.record_hash
        """,
        set={
            "is_current": lit("N"),
            "End_date": current_date()
        }
    ).whenMatchedInsert(
        condition=""" 
            target.record_hash <> source.record_hash
        """,
        values={
            "student_id": col("source.student_id"),
            "student_first_name": col("source.student_first_name"),
            "student_last_name": col("source.student_last_name"),
            "student_email": col("source.student_email"),
            "student_gender": col("source.student_gender"),
            "student_phone_numbers": col("source.student_phone_numbers"),
            "student_address": col("source.student_address"),
            "CurrentFlag": lit("Y"),
            "start_date": current_date(),
            "End_date": None,
            "record_hash": col("source.record_hash")
        }
    ).whenNotMatchedInsert(
        values={
            "student_id": col("source.student_id"),
            "student_first_name": col("source.student_first_name"),
            "student_last_name": col("source.student_last_name"),
            "student_email": col("source.student_email"),
            "student_gender": col("source.student_gender"),
            "student_phone_numbers": col("source.student_phone_numbers"),
            "student_address": col("source.student_address"),
            "CurrentFlag": lit("Y"),
            "start_date": current_date(),
            "End_date": None,
            "record_hash": col("source.record_hash")
        }
    ).execute()

# Second level DAG 
# Scenario 2-lvel: DeltaTable and dataframe mostly for streaming scenario as your data is arriving you are loading
def merge_scenario_1():
    delta_table.alias("target").merge(
            source = dataframe_events().alias("events"),
            condition = "target.record_hash = source.record_hash"
        ).whenNotMatchedInsertAll() \
         .execute()

# Example usage
# Assume source_df is a DataFrame loaded with the incoming data that needs to be merged.
# merge_scd_type_2("/path/to/delta/table", source_df)

    
if __name__ == "__main__":
    # help(delta_table.merge)
    #dataframe_events()
    merge_scenario_1()
    # merge_scenario_2()
    # partial_merge()
    #scd_merge()


